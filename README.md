# FCSE-Skopje 2023 Undergraduate Study Programs Ingestor

The Ingestor application is used to load the preprocessed data of undergraduate study programs and courses at
the [Faculty of Computer Science and Engineering](https://finki.ukim.mk) at
the [Ss. Cyril and Methodius University in Skopje](https://www.ukim.edu.mk).
The data is first processed by the preprocessor component and stored in an Iceberg table format.  
This application loads that processed data into a Neo4j database using the **asynchronous Neo4j Python driver**.

## Requirements

- **Data Source:** This application requires the raw data output generated by the
    **[undergraduate-study-program-preprocessor](https://github.com/username-gigo-is-not-available/undergraduate-study-programs-preprocessor)**.
- **Python 3.9 or later**.
- Access to an **Iceberg Catalog** and **MinIO server** (for S3 storage) or local disk space (for local storage).
- Access to a **Neo4j Database**

## Overview

The Ingestor application connects to an Iceberg warehouse and asynchronously ingests nodes and relationships into Neo4j.  
It uses a batching and partitioning strategy to enable safe and scalable concurrent writes.

### Data Model

#### Nodes

- `StudyProgram(uid, code, name, duration, url, offers(Curriculum))`
- `Course(uid, code, name_mk, name_en, level, included_by(Curriculum), requires(Requisite), satisfies(Requisite), taught_by(Professor))`
- `Professor(uid, name, surname, teaches(Course))`
- `Curriculum(uid, type, semester_season, academic_year, semester, offered_by(StudyProgram), includes(Course))`
- `Requisite(uid, type, minimum_number_of_courses_required, required_by(Course), satisfies_by(Course))`

#### Relationships

- `OFFERS(StudyProgram -> Curriculum)`
- `INCLUDES(Curriculum -> Course)`
- `REQUIRES(Course -> Requisite)`
- `SATISFIES(Course -> Requisite)`
- `TEACHES(Professor -> Course)`

### Partitioning Strategy

To reduce lock contention and enable concurrent writes, each record is assigned a `partition_uid` based on the last characters
of the source and destination node IDs. A wrap-around partitioning strategy is used to ensure each batch touches disjoint node subsets.

Each batch forms Cypher `UNWIND` write operations executed asynchronously against Neo4j.

### Pipeline

This pipeline reads the Iceberg tables generated by the Preprocessor
and performs the following steps for each entity:
---
#### Nodes

| Entity         | Action                                                                 |
|----------------|------------------------------------------------------------------------|
| StudyProgram   | Create study program nodes                                             |
| Course         | Create course nodes                                                    |
| Professor      | Create professor nodes                                                 |
| Curriculum     | Create curriculum nodes                                                |
| Requisite      | Create requisite nodes                                                 |

#### Relationships

| Relationship | Source → Target     | Action                                                   |
|--------------|-------------------|---------------------------------------------------------|
| OFFERS       | StudyProgram → Curriculum | Create `OFFERS` relationships                     |
| INCLUDES     | Curriculum → Course      | Create `INCLUDES` relationships                   |
| REQUIRES     | Course → Requisite       | Create `REQUIRES` relationships                   |
| SATISFIES    | Course → Requisite       | Create `SATISFIES` relationships                  |
| TEACHES      | Professor → Course       | Create `TEACHES` relationships                    |

---

## Installation and Setup

1. **Clone the repository**
   ```bash
   git clone <repository_url>
   ```

2. **Install the required packages**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configuration Files**
   Ensure you have the necessary environment configuration. This typically involves:
    * A **`.env`** file for environment variables.
    * A **`.pyiceberg.yaml`** file for Iceberg catalog configuration.

---

## Environment Variables

The application is configured using environment variables, which define storage access and Iceberg table names.

### General Configuration

| Variable         | Description                                                                                               |
|:-----------------|:----------------------------------------------------------------------------------------------------------|
| `FILE_IO_TYPE`   | Storage type for the lakehouse metadata. Must be **`local`** or **`s3`**.                                 |

### Iceberg Configuration (Metastore)

| Variable                        | Description                                                                                               |
|:--------------------------------|:----------------------------------------------------------------------------------------------------------|
| `PYICEBERG_HOME`                | The internal path where the application is executed (e.g., `/undergraduate-study-programs-preprocessor`). |
| `ICEBERG_CATALOG_NAME`          | The name of the catalog configuration (e.g., `default`) used to connect to the metastore.                 |
| `ICEBERG_SOURCE_NAMESPACE`      | The database/schema where **raw input data** is located (e.g., `raw`).                                    |
| `ICEBERG_DESTINATION_NAMESPACE` | The database/schema where **processed output data** will be written (e.g., `processed`).                  |

### Storage-Specific Configuration

#### 1. Local Storage (`FILE_IO_TYPE="LOCAL"`)

| Variable                            | Description                                                                                                      |
|:------------------------------------|:-----------------------------------------------------------------------------------------------------------------|
| `LOCAL_ICEBERG_LAKEHOUSE_FILE_PATH` | The **absolute path** to the root directory that serves as the Iceberg warehouse (e.g., `/app/local_lakehouse`). |

#### 2. MinIO/S3 Storage (`FILE_IO_TYPE="S3"`)

| Variable                           | Description                                                                                                                |
|:-----------------------------------|:---------------------------------------------------------------------------------------------------------------------------|
| `S3_ENDPOINT_URL`                  | The full endpoint URL (host and port) of the MinIO server (e.g., `localhost:9000`).                                        |
| `S3_ACCESS_KEY`                    | The access key ID required for MinIO authentication.                                                                       |
| `S3_SECRET_KEY`                    | The secret access key required for MinIO authentication.                                                                   |
| `S3_ICEBERG_LAKEHOUSE_BUCKET_NAME` | The name of the S3 bucket that will host the Iceberg warehouse (e.g., `finki-warehouse`).                                  |
| `S3_PATH_STYLE_ACCESS`             | Boolean flag (`True`/`False`) indicating whether to use path-style addressing (required for MinIO or custom S3 endpoints). |

---

### Dataset Naming Configuration

These variables allow flexibility in naming the 10 resulting Iceberg tables:

| Variable                      | Description                                                       |
|:------------------------------|:------------------------------------------------------------------|
| `STUDY_PROGRAMS_DATASET_NAME` | Output table name for study programs (e.g., `study_programs`).    |
| `COURSES_DATASET_NAME`        | Output table name for courses (e.g., `courses`).                  |
| `CURRICULA_DATASET_NAME`      | Output table name for curriculum details (e.g., `curricula`).     |
| `REQUISITES_DATASET_NAME`     | Output table name for requisites (e.g., `requisites`).            |
| `PROFESSORS_DATASET_NAME`     | Output table name for professors (e.g., `professors`).            |
| `OFFERS_DATASET_NAME`         | Output table name for offers relationship (e.g., `offers`).       |
| `INCLUDES_DATASET_NAME`       | Output table name for includes relationship (e.g., `includes`).   |
| `REQUIRES_DATASET_NAME`       | Output table name for requires relationship (e.g., `requires`).   |
| `SATISFIES_DATASET_NAME`      | Output table name for satisfies relationship (e.g., `satisfies`). |
| `TEACHES_DATASET_NAME`        | Output table name for teaches relationship (e.g., `teaches`).     |

---
### Neo4j Configuration

#### Database Connection

| Variable              | Description                                                                                   |
|:---------------------|:----------------------------------------------------------------------------------------------|
| `DATABASE_HOST_NAME` | Hostname or IP address of the Neo4j instance (e.g. `neo4j`, `localhost`).                      |
| `DATABASE_PORT`      | Bolt port Neo4j is listening on (default: `7687`).                                             |
| `DATABASE_NAME`      | Target database name inside Neo4j (e.g., `neo4j`, `study_programs`).                           |
| `DATABASE_USER`      | Username for Neo4j authentication.                                                             |
| `DATABASE_PASSWORD`  | Password for Neo4j authentication.                                                             |

#### Connection Pool

| Variable                                   | Description                                                                                               |
|:-------------------------------------------|:----------------------------------------------------------------------------------------------------------|
| `DATABASE_CONNECTION_ACQUISITION_TIMEOUT`  | Max time (seconds) to wait for a connection from the pool before raising an error.                        |
| `DATABASE_CONNECTION_TIMEOUT`              | Timeout (seconds) for establishing a physical network connection.                                          |
| `DATABASE_MAX_CONNECTION_LIFETIME`         | Max lifetime (seconds) of a pooled connection before being recycled.                                       |
| `DATABASE_MAX_CONNECTION_POOL_SIZE`        | Maximum number of concurrent active connections allowed in the pool.                                       |
| `DATABASE_MAX_TRANSACTION_RETRY_TIME`      | Maximum time (seconds) the driver will retry failed transactions due to transient errors.                  |

#### Retry / Backoff Strategy

| Variable                               | Description                                                                                                       |
|:---------------------------------------|:------------------------------------------------------------------------------------------------------------------|
| `DATABASE_RETRY_COUNT`                 | Number of retry attempts for failed transactions.                                                                 |
| `DATABASE_RETRY_MULTIPLIER_IN_SECONDS` | Initial delay used in exponential backoff between retries (e.g. 0.5 → 500ms).                                     |
| `DATABASE_RETRY_EXPONENT_BASE`         | Exponential factor for backoff (e.g. `2` doubles the wait time after each retry).                                 |

---
## Running the Ingestor

### Option 1: Local Execution

Ensure all environment variables are loaded (e.g., `source .env` on Linux/macOS or equivalent in PowerShell).

```bash
python run.py
```
### Option 2: Docker Compose (Recommended for S3/MinIO)

If your setup uses MinIO, use Docker Compose to manage both the Python application and the supporting services.

```bash
docker compose up
```